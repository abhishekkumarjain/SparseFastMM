%% This is file `sample-manuscript.tex', generated with the docstrip
%% utility.  The original source files were: samples.dtx (with
%% options: `manuscript') IMPORTANT NOTICE: For the copyright see the
%% source file.  Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.  For
%% distribution of the original source see the terms for copying and
%% modification in the file samples.dtx.  This generated file may be
%% distributed as long as the original source files, as listed above,
%% are part of the same distribution. (The sources need not
%% necessarily be in the same archive or directory.)  The first
%% command in your LaTeX source must be the \documentclass command.
\documentclass[manuscript,screen]{acmart}
\usepackage{bm}
\usepackage{lineno}
\linenumbers
% Paolo's definition. Yes, I have my way to define stuff. Please take
% a look



% Always include hyperref last
%\usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}
%\graphicspath{{../python/PNG/}}
\input{mydef}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2020}
\acmYear{2020}
\acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Randomization of Sparse Matrix by Vector Multiplication }

\author{Abhishek Jain}
%\email{---}
\author{Ismail Bustany}
%\email{---}
\author{Paolo D'Alberto}
%\email{---}

%\affiliation{%
%  \institution{1 * -}
%  \streetaddress{2 * -}
%  \city{3 * -}
%  \state{4 * -}
%  \postcode{5 * -}
%}

\renewcommand{\shortauthors}{Jain et al.}

\begin{abstract}
A sparse matrix by vector multiplication (SpMV) is simplified by the
matrix non-zero elements and how we store them. There are many SpMV
applications, many matrix storage formats, and thus
algorithms. However, there is no optimality without considering the
architecture: for example, the CPU is one among many.

By nature, randomization is resilient to counter techniques, thus
suitable to avoid worst case scenarios because we tend to reduce to an
average case; however, randomization does to the best case scenario
the same thing it does to the worst case, it can nudge the optimal
solution off. Like preconditioning, randomization is advantageous when
the matrix is reused or a constant such as in the power method,
Krilov's space, or convolutions for image classifications. Differently
from preconditioning we do not change the values of the matrix, we
randomize row and column of the matrix. We shall show that
randomization is an optimization that any architecture may take
advantage although in different ways. Most importantly, any developer
can consider and deploy. We shall present cases where we can improve
performance by 15\% on AMD-based systems.

\end{abstract}

\maketitle

\section{Introduction} 
\label{sec:introduction}

The obious questions are what is randomization and why would we use
it? We shall provide formal definitions in the following sections, in
this context, we randomly permute rows and column of a sparse matrix
before a (sparse) matrix by a (dense) vector operation. We do this
because randomization is the poor man's preconditioning and we do not
mean it in a pejorative sense.

Preconditioning is a method to help the convergence of an iterative
solution, for example a sequence of matrix by vector operations.  Each
iteration does a better job in searching the space and converging to a
solution. In general, it means better numerical properties and well
defined properties of the matrix itself. It does not mean that each
iteration is faster.  We want to make each iteration faster. From a
mathematical and scientific point of view may seem uninteresting. From
the engineering and deployment point of view is just the beginning.

There is a common thread in the scientific community to faster
computations: multi-core systems. These are composed by multi-cores
processors and GPUs. The main goal is a balanced work distribution
and, when applicable, minimal communication
\cite{DBLP:journals/siamsc/KayaaslanAU18,DBLP:conf/ieeehpcs/PageK18}. When
storage strategy and algorithms must be considered together then GPUs
provide the work horse for the current trust and research
\cite{DBLP:journals/topc/AnztCYDFNTTW20}. This research is towards
optimal solutions and the authors strive for a clear and complete
understanding of the software--hardware relation, and usually the
hardware is composed of symmetric computational units. Interestingly,
the SpMV's space and time complexity, which are small, may not warrant
more performance because we end up using only one thousandth the
capacity of the hardware. We may deploy efficient solutions: not
necessarily faster but overall tailored for this. 
  
The peak performance of any SpMV accelerator depends primarily on the
available memory bandwidth (i.e., DRAM such as DDR or HBM) and the
capability of the accelerator to effectively use it.  Because SpMV is
memory-bound, a more important metric than peak performance alone is
the fraction of bandwidth utilized, which captures the overall
efficiency of the architecture. GPU platforms exhibit very high
bandwidth, see the experimental Section \ref{sec:experimentalresults}:
Ellesmere DDR5 224GB/s, Fiji HBM 512GB/s, and Vega 20 HBM 1TB/s.
Although utilizing this much bandwidth efficiently is difficult for
large scale and highly sparse matrices due to very high random access
pattern.  Custom architectures based on FPGA or ASIC devices can
maximize bandwidth utilization by highly customized data-paths and
memory hierarchy designs {\bf MISSING CITATION \cite{} }. Most of the
existing accelerators saturates relatively low memory bandwidth
available on FPGA platforms (less than 80 GB/s) {\bf MISSING CITATION
  \cite{} }.  Modern FPGA platforms have multiple HBM stacks to
provide large memory bandwidth. However there is no implementation
(currently available) that saturates all of the available DRAM
bandwidth for SpMV kernel on HBM-enabled FPGA platforms. Scalability
of accelerator design remains a major concern and it is an active area
of research.

FPGA platforms used in early works exhibit low peak performance due to
the scarcity of external memory bandwidth.  For example, Microsoft's
implementation of SpMV uses an FPGA platform which has only 2 DDR2-400
memory with a resulting bandwidth of 6.4 GB/s {\bf MISSING CITATION
  \cite{}}.  The accelerator is running at 100 MHz, it reads 64 Bytes
of data every cycle, which corresponds to 5 non-zeros every cycle (a
non-zero is about 12 Bytes). At best, the peak performance is 10
double precision operations every cycle at 100 MHz, which is 1 GFLOPS
(only). In 2009, The Convey systems released Convey HC-1 FPGA platform
with 16 DDR2-677 memories resulting in overall 80 GB/s memory
bandwidth {\bf MISSING CITATION \cite{} }. The accelerator logic is
allowed to run at 150 MHz, it consumes 512 Bytes of data every cycle,
which corresponds to around 40 non-zeros every cycle.  At best, the
peak performance is 80 double precision operations every cycle at 150
MHz, which is 12 GFLOPS.

One of the key building block for custom solutions is a multi-ported
buffer used to store vector entries.  During execution, multiple
column indexes are used as addresses to read corresponding vector
entries; we shall provide more details in Section \ref{sec:notations}.
Designing a buffer with very large number of read ports is
challenging.  One solution is {\em banking} as a mechanism to store
partitioned vector entries.  Although banking could allow very high
throughput indexing unless the same entry is required multiple times
and its reads are purely sequential and loss of bandwidth.  For
example, hashing techniques and data duplication are possible
solutions. However another problem arise: when we distribute SpMV
computations across $p$-nodes, some of the nodes finish early and $k$
nodes finish later because of unbalanced load in row/column major
traversal. This is common for matrices where few rows or columns are
dense. The $k$ nodes out of $p$ finish late and refer to as {\em
  straggler nodes}.  Using random permutation of column/row we are
trying to balance the load across $p$ workers so that there is no
straggler. From this hardware prospective, randomization is an
optimization and provide a clear context our current work.
 
At this stage, we have too many nobs and tools to tune: algorithms,
data structures, and dedicated hardware (CPU, GPUs, Custom). This is a
(very) hard problem and we are not here for the solution of the
inverse problem: find the best Hardware-Software solution for the one
matrix by vector product. We are here to provide tools, we may say
naive tools, to help understand how the structure of the matrix may
affect the HW-SW solution. Randomization, or versions of it, is
already used by custom hardware to re-organize the data flow to reduce
communications and computation bottle necks. We come to play in this
arena to show {\em how} to use randomization if at all.

For the readers in the field of algorithms, sparse matrix by (dense)
vector is basically a sorting algorithm. Bare with us, Sorting is a
method to find if an element is in a list without prior or limited
knowledge of the list contents. Sorting is used to prepare the matrix
and to find elements in between sparse matrices and sparse vectors. In
custom architectures, sorting networks are used for routing elements
of the matrix and vector to the proper functional unit. Interestingly,
The best sorting algorithm is a function of the distribution of
elements. If you are stuck with a sorting algorithm and the wrong
distribution, randomization may change the distribution, and you do
not need to talk to any HW designer

We organize our work as follows: In Section \ref{sec:notations}, we
define the matrix by vector operation; in Section
\ref{sec:randomization}, we define what we mean for randomization. We
use randomization to create a uniform distribution in Section
\ref{sec:uniform} and we measure uniformity by nothing else than
entropy in Section \ref{sec:entropy}. We present how we drive our
experiments to show the effects of randomization in Section
\ref{sec:measuring}. In the last sections we present a summary of the
results: we present our work loads, benchmarks, in Section
\ref{sec:workload}, and the complete set of measures for an AMD CPU
and GPUs system in Section \ref{sec:experimentalresults} .

\section{Basic Notations}
\label{sec:notations}
Let us start by describing the basic notations so we can clear the
obvious (or not).  A Sparse-matrix by vector multiplication {\em SpMV}
on an (semi) ring based on the operations $(+,*)$ is defined as
$\Vc{y} = \M \Vc{x}$ so that $y_i = \sum_j M_{i,j}*y_j$ where $M_{i,j}
\eq 0$ are not even represented and stored. Most of the experimental
results in Section \ref{sec:experimentalresults} are based on the
classic addition (+) and multiplication (*) in floating point
precision using 64bits (i.e., double floating point precision).  SpMV
based on semi-ring (min,+) is a short path algorithm based on an
adjacent matrix of a graph, and using a Boolean algebra we can check
if two nodes are connected, which is slightly simpler.

We identify a sparse matrix $\M$ of size $M\times N$ as having
$O(M+N)$ non-zero elements, number of non zero {\em nnz}. Thus the
complexity of $\M \Vc{x}$ is $O(M+N) = 2nnz$. Of course, the
definition of sparsity may vary. We represent the matrix $\M$ by using
the Coordinate {\em COO} or and the compressed sparse row {\em
  CSR}\footnote{a.k.a. Compressed row storage {\rm CRS}.}  format. The
COO represents the non-zero of a matrix by a triplet $(i,j,val)$, very
often there are three identical-in-size vectors for the ROW, COLUMN,
and VALUE. The COO format takes $3\times nnz$ space and two
consecutive elements in the value array are not bound to be neither in
the same row nor column. In fact, we know only that $VALUE[i] =
M_{ROW[i],COLUMN[i]}$.

The CSR stores elements in the same row and with increasing column
values consecutively. There are three arrays V, COL, and ROW. The ROW
is sorted in increasing order, its size is $M$, and $ROW[i]$ is an
index in V and COL describing where row-$i$ starts (i.e., if row $i$
exists).  We have that $M_{i,*}$ is stored in $V[ROW[i]:ROW[i+1]]$ and
the column are at $COL[ROW[i]:ROW[i+1]]$ and sorted increasingly. The
CSR takes $2\times nnz + M$ space and a row vector of the matrix can
be found in $O(1)$.
 
The computation as $y_i = \sum_j M_{i,j}*x_j$ is a sequence of dot
products and the CSR representation is a natural:

\[ Index = ROW[i]:ROW[i+1] \]
\[
y_i =  \sum_{\ell\in Index} V[\ell] * x_{COL[\ell]}  
\]
The matrix row is contiguous (in memory) and contiguous rows are
contiguous. The access of the (dense) vector $\Vc{x}$ could have no
pattern. The COO format could use a little preparation: For example,
we can sort the array by row and add row information to achieve the
same properties of CSR; however transposing a COO matrix is just a
swap of the array ROW and COL. Think about matrix multiply. As today,
each dot product achieves peak performance if the reads of the vector
$\Vc{x}$ are streamlined as much as possible and so the reads of the
vector $V$. If we have multiple cores, each could compute a sub set of
the $y_i$ and a clean data load balancing can go a long way. If we
have a few functional units, we would like to have a constant stream
of independent $*$ and $+$ operations but with data already in
registers: that is, data pre-fetch will go a long way especially for
$x_{COL[i]}$, which may have an irregular pattern.


\section{Randomization}
\label{sec:randomization}
We refer to {\em Randomization} as row or column permutations of the
matrix $\M$ (thus a permutation of $\Vc{y}$ and $\Vc{x}$) and we choose
these by a pseudo-random process. Why we want to introduce
uncertainty? The sparsity of our matrix $\M$ has a pattern
representing the nature of the original problem; such a pattern may
exploit the wrong computation for an architecture; we could break such
a pattern so that the only property left is a uniform distribution (of
some sort). We must avoid the worst case and we would opt for an
average case instead and we could do this to a class of $\M$. 

If we know the matrix $\M$ and we know the architecture,
preconditioning must be a better solution.  Well, it is. If we run
experiments long enough, we choose the best permutations for the
architecture, permute $\M$, and go on testing the next.  On one end,
preconditioning exerts a full understanding of both the matrix (the
problem) and how the final solution will be computed
(architecture). This is the culminating point of knowing and we must
strive to it. On the other end, the simplicity of a random permutation
requires no information about the matrix, the vector, and the
architecture. Such a simplicity can be exploited directly in HW. We
are after an understanding when randomization is just enough: we want
to let the hardware do its best with the least effort, or at least
with the appearance to be effortless. Also we shall show there are
different flavors of random.


Interestingly, this work stems from a sincere surprise about
randomization efficacy and its application on custom SpMV. Here, we
want to study this problem systematically so that to help future
hardware designs. Intuitively, if we can achieve a uniform
distribution of the rows of matrix $\M$ we can have provable
expectation of its load balancing across multiple cores. If we have a
uniform distribution of accesses on $\Vc{x}$ we could exploit column
load balancing and exploit better sorting algorithms: in practice the
reading of $\Vc{x}_{COL[i]}$ can be reduces to a sorting and we know
that different sparsity may require different algorithms. This is a
lot to unpack but this translates to a better performance of the
sequential algorithm without changing the algorithm or better HW
utilization.


We will show that (different) randomness affects architectures and
algorithms differently, making randomization a suitable optimization
especially when the application and hardware are at odds, hardware is
difficult to change and the matrix sparsity is simple to change. We
want to show that there is a randomness hierarchy that we can
distinguish as global and local; there are simple-to-find cases where
the sparsity breaks randomness and the matrix has to be split into
components.  We want to show that this study uses common tool, open
software tools and sometimes naive experiments; however, we can infer
properties applicable to proprietary and custom solutions.

\newpage 
\Doublefigure{.49}{../python/PNG/OPF_3754_mtx_regular}{../python/PNG/lp_osa_07_mtx_regular}{Left:
  OPF 3754. Right: LP OSA 07. These are histograms where we represent
  normalized buckets and counts}{fig:one}

\section{Entropy}
\label{sec:entropy}
Patterns in sparse matrices are often visually pleasing, see Figure
\ref{fig:one} where we present the height histogram, the width
histograms and a two-dimensional histogram as heat map. We will let
someone else using AI picture classification. Intuitively, we would
like to express a measure of uniform distribution and here we apply
the basics: {\em Entropy}. Given an histogram $i\in[0,M-1]$ $h_i \in
\N$, we define $S =\sum_{i=0}^{M-1}h_i$ and thus we have a probability
distribution function $p_i = \frac{h_i}{S}$. The {\em information} of
bin $i$ is defined as $I(i) = -\log_2 p_i$. If we say that the
stochastic variable $X$ has PDF $p_i$ than the entropy of $X$ is
defined as.

\begin{equation}
  \label{eq:entropy}
  H(x) = -\sum_{i=0}^{M-1} p_i\log_2p_i = \sum_{i=0}^{M-1}p_i I(i) =
  E[I_x]
\end{equation}
The maximum entropy is when $\forall i, p_i = p = \frac{1}{M}$; that
is, we are observing a uniform distributed event. There is no
conceptual difference when the PDF represents a two dimensional
distribution. Thus our randomization should aim at higher entropy
numbers. The entropy for matrix LP OSA 07 is 8.41 and for OPF 3754 is
8.39. We use the entropy specified in the Scipy stats module.  A
single number is concise and satisfying. If you are pondering why they
are so close contrary to their sparsity we discuss this next.


\section{Uniform distribution}
\label{sec:uniform}
We know that we should {\bf not} compare the entropy numbers of two
matrices because entropy does not use any information about the order
of the buckets only their probabilities. By construction, the matrices
are quite different in sparsity and in shapes, however their entropy
numbers are very close.  Two matrices with the same number of
non-zeros, spaced well enough in the proper number of bin, will have
the same entropy. To appreciate their different sparsity, we should
compare their entropy distributions by Jensen-Shannon measure (which
is a symmetric measure, please do not use Kullback-Leibler KL
divergence) \cite{dalberto2012nonparametric}. Or we could use a representation of a hierarchical
2d-entropy, see Figure \ref{fig:two}, where the entropy is split into
2x2, 4x4 and 8x8 (or fewer if the distribution is not square). We have
a hierarchical entropy heat maps.

\doublefigure{.40}{../python/ENTOPF3754/2d-regular}{../python/ENTlposa07/2d-regular}{Hierarchical
  2D entropy for OPF 3754 (left) and LP OSA 07 (right). }{fig:two}

We can see that a granular entropy summarizes better the nature of the
matrix because it keep some spatial information. In this work, the
entropy vector is used mostly for visualization purpose more than for
comparison purpose. Of course, we can appreciate how the matrix LP OSA
07 has a few very heavy rows and they are clustered. This matrix will
help us showing how randomization need some tips. Now we apply row and
column random permutation once by row and one by column: Figure
\ref{fig:three}: OPF has now entropy 11.27 and LP 9.26. The numerical
difference is significant. The good news is that for entropy, being an
expectation, we can use simple techniques like bootstrap to show that
the difference is significant or we have shown that Jensen-Shannon can
be used and a significance level is available. What we like to see is
the the hierarchical entropy heat map is becoming {\em more} uniform
for at least one of the matrix.

\doublefigure{.40}{../python/ENTOPF3754/2d-row-column-shuffle}{../python/ENTlposa07/2d-row-column-shuffle}{Hierarchical
  2D entropy after row and column random permutation for OPF 3754
  (left) and LP OSA 07 (right). }{fig:three}

In practice, permutations need some help especially for relatively
large matrices. As you can see, the permutation affects locally the
matrix. Of course, it depends on the implementation of the random
permutation (we use numpy for this) but it is reasonable a slightly
modified version of the original is still a random selection but
unfortunately they seem more likely than they should. We need to
compensate or help the randomization so that this current
implementation does not get too lazy.

If we are able to identify the row and column that divide high and low
density, we could use them as pivot for a shuffle like in a quick-sort
algorithm. We could apply a sorting algorithm but its complexity will
the same of SpMV. We use a gradients operations to choose the element
with maximum steepness, Figure \ref{fig:four} and \ref{fig:five}

LP achieves entropy 8.67 and 9.58 and OPF achieves 10.47 and 11.40.

\doublefigure{.40}{../python/ENTOPF3754/2d-H-shuffle}{../python/ENTlposa07/2d-H-shuffle}{Hierarchical
  2D entropy after height gradient based shuffle and row random
  permutation for OPF 3754 (left) and LP OSA 07 (right). }{fig:four}

\doublefigure{.40}{../python/ENTOPF3754/2d-W-shuffle}{../python/ENTlposa07/2d-W-shuffle}{Hierarchical
  2D entropy after height and width gradient shuffle and row and
  column random permutation for OPF 3754 (left) and LP OSA 07
  (right). }{fig:five}

If the goal is to achieve a uniformly sparse matrix, it seems that we
have the tools to compute and to measure such a sparsity. We admit
that we do not try to find the best permutation. But our real goal is
to create a work bench where randomization can be tested on different
architectures and different algorithms. A randomization with a
measurable uniform distribution is preferable than just random. We are
interested to find out when random is enough or not enough. Also,
consider that to achieve a uniform distribution, we do not need a
random transformation and any permutation balancing the number of
non-zero is possible, but for now not looked for.

\section{Measuring the randomization effects}
\label{sec:measuring}

Whether or not this ever applied to the reader, when we have timed
algorithms (i.e., measure execution time), we came to expect
variation.  The introduction of randomization may hide behind the ever
present variance, after all these are algorithms on {\em small} inputs
and small error can be comparable to the overall execution time. Here,
we must address this concern even before describing the experiments.

First, we execute every algorithm between 1000 and 5000 times. The
time of each experiment is in the seconds, providing a granularity for
which we are confident the measuring time error is under
control. Thus, for each experiment we provide an average execution
time: we measure the time and we divide by the number of trials. Cold
starts, the first iteration, are still accounted. To make the measure
portable across platform we present GFLOPS, that is, Giga ($10^{12}$)
floating operations per second: $2*nnz$ divided by the average time in
seconds.

Then we repeat the same experiment 32 times. Permutations in {\em
  numpy} Python uses a seed that is time sensitive: thus every
experiment is independent from the previous. The number 32 is an old
statistic trick and it is a minimum number of independent trials to
approximate a normal distribution. In practice, they are not but the
number is sufficient for most of the cases and it is an excellent
starting point.

A short hand legend: {\bf Reg} is the matrix without any permutation
and thus is the regular; {\bf R} stands for random Row permutation;
{\bf G-R} stands for gradient-based row shuffle and random row
permutation; {\bf G-C} stands for gradient-based column shuffle and
random column permutation; {\bf R-C} stands for random row and column
permutation.  This legend is used in the pictures to be concise, in
the tables in the following sections, we use a verbose description. We
shall clarify the gradient based approach in the experimental results
section \ref{sec:experimentalresults}. Intuitively, we help the random
permutation by a quick targeting of high and low volume of the
histogram (and thus the matrix).


In Figure \ref{fig:five1}, We show CPU performance using COO and CSR
SpMV algorithms for the matrix OPF 3754. We can see that the CSR
algorithms are consistent and the Regular (i.e., the original) has
always the best performance. For the COO, permutations introduce long
tails, thus performance advantage. 
\doublefigure{.45}{../python/PNG2/OPF_3754_mtx_CPU_COO.png}{../python/PNG2/OPF_3754_mtx_CPU_CSR.png}{CPU COO (left) and CPU CSR (left) for OPF 3754}{fig:five1}

In Figure \ref{fig:six}, \ref{fig:six1} and \ref{fig:six2},
randomization is harmful to the GPU implementation. The OPF 375 matrix
is mostly diagonal, thus the vector $\Vc{x}$ is read in close
quarters, randomization breaks it.  If the load balance is fixed
(i.e., by dividing the matrix by row and in equal row), randomization
is beneficial.

\doublefigure{.45}{../python/PNG2/OPF_3754_mtx_GPU_64_COO.png}{../python/PNG2/OPF_3754_mtx_GPU_64_CSR.png}{Vega
  20, GPU 64bits COO (left) and GPU CSR (right) for OPF 3754}{fig:six}
\doublefigure{.45}{../python/ELLEPNG/OPF_3754_mtx_GPU_64_COO.png}{../python/ELLEPNG/OPF_3754_mtx_GPU_64_CSR.png}{Ellesmere, GPU 64bits COO (left) and GPU CSR (right) for OPF 3754}{fig:six1}
\doublefigure{.45}{../python/FIJIPNG/OPF_3754_mtx_GPU_64_COO.png}{../python/FIJIPNG/OPF_3754_mtx_GPU_64_CSR.png}{Fiji, GPU 64bits COO (left) and GPU CSR (right) for OPF 3754}{fig:six2}

If we take the original matrix and split into part having the same
number of rows, and execute them in parallel using different cores, we
can see in Figure \ref{fig:seven} that randomization is quite useful.
\singlefigure{.45}{../python/PNG2/OPF_3754_mtx_CPU_PAR.png}{ Parallel
  CPU CSR for OPF 3754}{fig:seven}


For matrix LP OSA 07, randomization helps clearly only for CPU CSR as
we show in Figure \ref{fig:eight}

\singlefigure{.45}{../python/PNG2/lp_osa_07_mtx_CPU_CSR.png}{   CPU CSR  for LP OSA 07}{fig:eight}

In Figure \ref{fig:eight1}, \ref{fig:eight2}, and \ref{fig:eight3}, we
can see that randomization is harmful but for one GPU, we can show
that a single exception is possible (40\% improvement).


\doublefigure{.45}{../python/PNG2/lp_osa_07_mtx_GPU_64_COO.png}{../python/PNG2/lp_osa_07_mtx_GPU_64_CSR.png}{Vega 20, GPU 64bits COO (left) and GPU CSR (right) for OPF 3754}{fig:eight1}
\doublefigure{.45}{../python/ELLEPNG/lp_osa_07_mtx_GPU_64_COO.png}{../python/ELLEPNG/lp_osa_07_mtx_GPU_64_CSR.png}{Ellesmere, GPU 64bits COO (left) and GPU CSR (right) for OPF 3754}{fig:eight2}
\doublefigure{.45}{../python/FIJIPNG/lp_osa_07_mtx_GPU_64_COO.png}{../python/FIJIPNG/lp_osa_07_mtx_GPU_64_CSR.png}{Fiji, GPU 64bits COO (left) and GPU CSR (right) for OPF 3754}{fig:eight3}


An example, the matrix MULT DCOP 01, is where randomization is useful
for the CPU, GPU, and the parallel version Figure \ref{fig:9},
\ref{fig:10} - \ref{fig:11} and the gains can be up to
10-15\%. Consider, we can achieve these improvements without any
insights to the architecture, the algorithms and their relationships.


\doublefigure{.45}{../python/PNG2/mult_dcop_01_mtx_CPU_COO.png}{../python/PNG2/mult_dcop_01_mtx_CPU_CSR.png}{CPU COO (left) and CPU CSR (right) for MULT DCOP 01}{fig:9}

\doublefigure{.45}{../python/PNG2/mult_dcop_01_mtx_GPU_64_COO.png}{../python/PNG2/mult_dcop_01_mtx_GPU_64_CSR.png}{Vega 20, GPU 64bits COO (left) and GPU CSR (right) for MULT DCOP 01}{fig:10}
\doublefigure{.45}{../python/ELLEPNG/mult_dcop_01_mtx_GPU_64_COO.png}{../python/ELLEPNG/mult_dcop_01_mtx_GPU_64_CSR.png}{Ellesmere,  GPU 64bits COO (left) and GPU CSR (right) for MULT DCOP 01}{fig:101}
\doublefigure{.45}{../python/FIJIPNG/mult_dcop_01_mtx_GPU_64_COO.png}{../python/FIJIPNG/mult_dcop_01_mtx_GPU_64_CSR.png}{Fiji, GPU 64bits COO (left) and GPU CSR (right) for MULT DCOP 01}{fig:102}

\singlefigure{.45}{../python/PNG2/mult_dcop_01_mtx_CPU_PAR.png}{ Parallel
  CPU CSR  for MULT DCOP 01 }{fig:11}

What does it mean when randomization does not work? The matrices we
use in this work are not chosen randomly (pun not intended), they are
the matrices that are difficult to handle in our custom SpMV engines
using a combination of sorting networks and systolic arrays. If
randomization does not work in our simplified work bench, will not
work in our specialized architecture because the reorganization of the
matrix or the input and output vector does not have the necessary
parallelism, data locality, and data streaming. We need to do
something else. In this case disrupting the memory pattern is not
sufficient. Thus, if we cannot beat the pattern, we must exploit it,
well not in this work.

\section{Workloads}
\label{sec:workload}

In the previous sections, we defined what we mean for randomization
and we present our tools of tricks for the measure of the effects of
randomization. Here we describe the work loads, the applications, we
use to test the effects of the randomization.

\subsection{Python COO and CSR algorithms}
\label{sec:pythoncoocsr}
The simplicity to compute the SpMV by the code $z = A*b$ in Python is
very rewarding. By change of the matrix storage format, $A =
A.tocsr(); z = A*b$, we have a different algorithm. The performance
exploitation is moved to the lower level.  The CSR implementation is
often two times faster but there are edge cases where the COO and COO
with randomization can go beyond and be surprisingly better: MUL DCOP
03 is an example where COO can do well.

Intuitively, Randomization can affect the performance because the
basic implementation is a sorting algorithm and it is a fixed
algorithm. There are many sorting algorithms and each can be optimal
for a different initial distribution. If we knew what is the sorting
algorithm we could tailor the input distribution. Here we just play
with it.

In Section \ref{sec:experimentalresults}, we present all the results
for CPU and GPUS. Keep in mind that these problems are hard, in the
sense they do not have fancy performance sheets (these architectures
can achieve Tera FLOPs sustained performance for dense computations).
If we go through diligently, we can see that there is a 15x
performance difference between the single thread CPU and Vega 20 GPU
(i.e, 3 vs 40 GFLOPS).

\subsection{Parallel CSR using up to 16 cores}
\label{sec:parcpu}
Python provides the concept of Pool to exploit a naive parallel
computation. We notice that work given to a Pool is split accordingly
to the number of elements to separate HW cores. We also noticed that
the work load move from a core to another, thus not ideal. Also we
notice that Pool introduce a noticeable overhead: a Pool of 1, never
achieves the performance of the single thread $z = A*b$. Using Pool
allows us to investigate how a naive row partitioning without counting
can scale up with number of cores. We tested by splitting the rows to
1--16 cores evenly (one thread per core) and we present the
performance for only the best configuration. The randomization goal is to
distribute the work uniformly: a balanced work distribution avoid the
unfortunate case where a single core does all the work. We are pleased
by the simplicity of the benchmark and we know we can do better.



\subsection{GPU COO and CSR algorithms}
\label{sec:gpucoocsr}
In this work, we use AMD GPUs and {\em rocSPARSE} is their current
software. The software has a few glitches but overall can be used for
different generation of AMD GPUs. We use the COO and CSR algorithms
and we provide performance measure for double precision only. The
ideas of using different GPUs: it is important to verify that the
randomization can be applied independently of the HW. We are not here
to compare performance across GPUs and CPUs. Often the limitation is
the software, how the software can exploit the hardware or how the
software will make easy to use a specific GPU. For example, the Fiji
architecture is clearly superior to the Ellesmere, however the latter
have better support and the system overall is more stable and user
friendly.

The performance of the CSR algorithm is about two times faster than the
COO. Most of the algorithms count the number of sparse elements in a
row and thus they can decide the work load partition
accordingly. Counting give you an edge but without changing the order
of the computation there could be cases where the work load is not
balanced and a little randomization could help and it does.

%\subsection{ FPGA ? (not necessary)}
\subsection{Randomization sometimes works}

For the majority of the cases we investigated and reported in the
following sections, Randomization does not work. However, there are
cases where randomization does work and does work for different
algorithms and architectures. If you are in the business of
preconditioning, permutations are pretty cheap. If you can find a good
one just consider like a preconditioning matrix, which it is. 

This shows also that HW has to be more conscious, well the HW designer
should, and accept that there are options at software level, at matrix
level and beyond. 


%\section{Call for a different strategy}
%\label{sec:strategy}
%We want to find out randomization techniques that are suitable for
%custom hardware but also what are the most common and simple
%heuristics that can justified for any hardware.


\section{Experimental Results}
\label{sec:experimentalresults}
The main hardware setup is a AMD Threadripper with 16 cores. We have
three Radeon GPUs: Vega 20 7nm, Pro 2xFiji, and Pro 2xEllesmere.

Vega 20 can deliver 3.5TFLOPS in double precision and it has 1TB/s HBM
memory. Each Fiji provides 0.5 TFLOPS in double precision and has
512GB/s HBM, the card has two chips.  The Ellesmere provides 0.3TFLOPS
in double precision and has 224GB/s DDR5, the card has two chips. In
the performance plots presented earlier and in the following, you will
notice that the performance gap between these GPUs is not so
marked. We can safely state that $vega \sim 2\times Fiji$ and $Fiji \sim
2\times ellesmere$



There are 4 basic randomization formats:
\begin{itemize}
  \item {\bf Random Row Permutation}, we take the original matrix and
    permute the rows.
  \item {\bf Random Row and Column Permutation}, we take the original
    matrix and permute the rows and the columns.
  \item {\bf Gradient based row permutation}, we compute the row
    histogram and we compute the gradient: $h_{i+1} - h_i$. We find a
    single point where the gradient is maximum, this is the pivot for
    a shuffle like a magician would shuffle a deck of cards.  Then we
    permute the two parts randomly.
  \item {\bf Gradient based row and column permutation}, As above but
    also for the columns.
\end{itemize}

For large matrices (large number of columns and rows) a permutation
tends to be a close variation of the original, still a random
permutation. The gradient allows us to describe two area of the
original matrix where there is a clear and de-marked density
variation: for example, there are two uniform distributed sub matrices
but one denser than the other. A shuffle redistribute every other
sample/card to different parts and these can be permuted locally.


    
We report in the following the performance results, we introduce a
{\bf *} following the best performance. This is tedious to read and,
we assure, to write. The code and the results are available as
software repository. \input{out.v.tex} \input{out.elle.tex}
\input{out.fiji.tex}



%\input{conclusion.tex}

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ACM-Reference-Format} \bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix{Review and Response}
%\input{review.tex}
\end{document}

%% This is file `sample-manuscript.tex', generated with the docstrip
%% utility.  The original source files were: samples.dtx (with
%% options: `manuscript') IMPORTANT NOTICE: For the copyright see the
%% source file.  Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.  For
%% distribution of the original source see the terms for copying and
%% modification in the file samples.dtx.  This generated file may be
%% distributed as long as the original source files, as listed above,
%% are part of the same distribution. (The sources need not
%% necessarily be in the same archive or directory.)  The first
%% command in your LaTeX source must be the \documentclass command.
\documentclass[manuscript,screen]{acmart}
\usepackage{bm}

% Paolo's definition. Yes, I have my way to define stuff. Please take
% a look



% Always include hyperref last
%\usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}
%\graphicspath{{../python/PNG/}}
\input{mydef}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2020}
\acmYear{2020}
\acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Randomization of Sparse Matrix by Vector Multiplication }

\author{Abhishek Jain}
%\email{---}
\author{Ismail Bustany}
%\email{---}
\author{Paolo D'Alberto}
%\email{---}

%\affiliation{%
%  \institution{1 * -}
%  \streetaddress{2 * -}
%  \city{3 * -}
%  \state{4 * -}
%  \postcode{5 * -}
%}

\renewcommand{\shortauthors}{Jain et al.}

\begin{abstract}
A sparse matrix by vector multiplication (SpMV) is simplified by the
matrix non-zero elements and how we store them. There are many SpMV
applications, many matrix storage formats, and thus
algorithms. However, there is no optimality without considering the
architecture: for example, the CPU is only one among ... many.

By nature, randomization is resilient to counter techniques, thus
suitable to avoid worst case scenarios, improve performance on
average, and reduce performance variance; however, it does to the best
case the same thing it does to the worst case, it can nudge it
off. Like preconditioning, randomization is advantageous when the
matrix is reused or a constant such as in the power method, Krilov's
space, or convolutions for image classifications.  Randomization is
also an optimization that any architecture may take advantage although
in different ways.

We shall present cases where we can improve by 15\% performance for
general purpose architectures and by 8x for custom architectures.

\end{abstract}

\maketitle

\section{Introduction} B.S. Goes here.
\label{sec:introduction}

\section{Basic Notations}
\label{sec:notations}
Let us start by describing the basic notations so we can clear the
obvious (or not).  A Sparse-matrix vector multiplication {\em SpMV} on
an (semi) ring based on the operations $(+,*)$ is defined as $\Vc{y} =
\M \Vc{x}$ so that $y_i = \sum_j M_{i,j}*y_j$ where $M_{i,j} \eq 0$
are not even represented and stored. Most of the experimental results
in Section \ref{sec:experimentalresults} are based on the classic
addition (+) and multiplication (*) in floating point precision using
32 or 64bits (i.e., single and double floating point precision).  SpMV
based on semi-ring (min,+) is a short path algorithm based on an
adjacent matrix of a graph, and using a Boolean algebra we can check
if two nodes are connected, which is slightly simpler. 



We identify a sparse matrix $\M$ of size $M\times N$ as having
$O(M+N)$ non-zero elements, number of non zero {\em nnz}. Thus the
complexity of $\M \Vc{x}$ is $O(M+N) = 2nnz$. Of course, the
definition of sparsity may vary. We represent the matrix $\M$ by using
the Coordinate {\em COO} or and the compressed sparse row {\em
  CSR}\footnote{a.k.a. Compressed row storage {\rm CRS}.}  format. The
COO represents the non-zero of a matrix by a triplet $(i,j,val)$, very
often there are three identical-in-size vectors for the ROW, COLUMN,
and VALUE. The COO format takes $3\times nnz$ space and two
consecutive elements in the value array are not bound to be neither in
the same row nor column. In fact, we know only that $VALUE[i] =
M_{ROW[i],COLUMN[i]}$.

The CSR stores elements in the same row and with increasing column
values consecutively. There are three arrays V, COL, and ROW. The ROW
is sorted in increasing order, its size is $M$, and $ROW[i]$ is an
index in V and COL describing where row-$i$ starts (i.e., if row $i$
exists).  We have that $M_{i,*}$ is stored in $V[ROW[i]:ROW[i+1]]$ and
the column are at $COL[ROW[i]:ROW[i+1]]$ and sorted increasingly. The
CSR takes $2\times nnz + M$ space and a row vector of the matrix can
be found in $O(1)$.
 
The computation as $y_i = \sum_j M_{i,j}*x_j$ is a sequence of dot
products and the CSR representation is a natural:

\[ Index = ROW[i]:ROW[i+1] \]
\[
y_i =  \sum_{i\in Index} V[i] * x_{COL[i]}  
\]
The matrix row is contiguous (in memory) and contiguous rows are
contiguous. The access of the (dense) vector $\Vc{x}$ could have no
pattern. The COO format could use a little preparation: For example,
we can sort the array by row and add row information to achieve the
same properties of CSR; however transposing a COO matrix is just a
swap of the array ROW and COL. Think about matrix multiply. As today,
each dot product achieves peak performance if the reads of the vector
$\Vc{x}$ are streamlined as much as possible and so the reads of the
vector $V$. If we have multiple cores, each could compute a sub set of
the $y_i$ and a clean data load balancing can go a long way. If we
have a few functional units, we would like to have a constant stream
of independent $*$ and $+$ operations but with data already in
registers: that is, data pre-fetch will go a long way especially for
$x_{COL[i]}$, which may have an irregular pattern.


\section{Randomization}
\label{sec:randomization}
We refer to {\em Randomization} as row or column permutations of the
matrix $\M$ (thus a permutation of $\Vc{y}$ and $\Vc{x}$) and we choose
these by a pseudo-random process. Why we want to introduce
uncertainty? The sparsity of our matrix $\M$ has a pattern
representing the nature of the original problem; such a pattern may
exploit the wrong computation for an architecture; we could break such
a pattern so that the only property left is a uniform distribution (of
some sort). We must avoid the worst case and we would opt for an
average case instead and we could do this to a class of $\M$. This is
the gist.

If we know the matrix $\M$ and we know the architecture,
preconditioning must be a better solution.  Well, it is. If we run
experiments long enough, we choose the best permutations for the
architecture, permute $\M$, and go on testing the next.  On one end,
preconditioning exerts a full understanding of both the matrix (the
problem) and how the final solution will be computed
(architecture). This is the culminating point of knowing and we must
strive to it. On the other end, the simplicity of a random permutation
requires no information about the matrix, the vector, and the
architecture. Such a simplicity can be exploited directly in HW. We
are after an understanding when randomization is just enough: we want
to let the hardware do its best with the least effort, or at least
with the appearance to be effortless. Also we shall show there are
different flavors of random.


Interestingly, this work stems from a sincere surprise about
randomization efficacy and its application on custom SpMV. Here, we
want to study this problem systematically so that to help future
hardware designs. Intuitively, if we can achieve a uniform
distribution of the rows of matrix $\M$ we can have provable
expectation of its load balancing across multiple cores. If we have a
uniform distribution of accesses on $\Vc{x}$ we could exploit column
load balancing and exploit better sorting algorithms: in practice the
reading of $\Vc{x}_{COL[i]}$ can be reduces to a sorting and we know
that different sparsity may require different algorithms. This is a
lot to unpack but this translates as better performance of the
sequential algorithm without changing the algorithm.

We will show that (different) randomness affects architectures and
algorithms differently making it a suitable optimization especially
when the application and hardware are at odds. We want to show that
there is a randomness hierarchy that we can distinguish as global and
local; there are simple-to-find cases where the sparsity breaks
randomness and the matrix has to be split into components.  We want to
show that this study uses common tool, open software tools and
sometimes naive experiments; however, we can infer properties
applicable to proprietary and custom solutions. 

\Doublefigure{.49}{../python/PNG/OPF_3754_mtx_regular}{../python/PNG/lp_osa_07_mtx_regular}{Left:
  OPF 3754. Right: LP OSA 07. These are histograms where we represent
  normalized buckets and counts}{fig:one}

\section{Entropy}
\label{sec:entropy}
Patterns in sparse matrices are often visually pleasing, see Figure
\ref{fig:one} where we present the height histogram, the width
histograms and a two-dimensional histogram as heat map. We will let
someone else using AI picture classification. Intuitively, we would
like to express a measure of uniform distribution and here we apply
the basics: {\em Entropy}. Given an histogram $i\in[0,M-1]$ $h_i \in
\N$, we define $S =\sum_{i=0}^{M-1}h_i$ and thus we have a probability
distribution function $p_i = \frac{h_i}{S}$. The {\em information} of
bin $i$ is defined as $I(i) = -\log_2 p_i$. If we say that the
stochastic variable $X$ has PDF $p_i$ than the entropy of $X$ is
defined as.

\begin{equation}
  \label{eq:entropy}
  H(x) = -\sum_{i=0}^{M-1} p_i\log_2p_i = \sum_{i=0}^{M-1}p_i I(i) =
  E[I_x]
\end{equation}
The maximum entropy is when $\forall i, p_i = p = \frac{1}{M}$; that
is, we are observing a uniform distributed event. There is no
conceptual difference when the PDF represents a two dimensional
distribution. Thus our randomization should aim at higher entropy
numbers.

The entropy for matrix LP OSA 07 is 8.41 and for OPF 3754 is 8.39. A
single number is satisfying because concise.  


\section{Uniform distribution}
\label{sec:uniform}
We know that we should {\bf not} compare the entropy numbers of two
matrices because there entropy does not use any information about the
order of the buckets. By construction, the matrices are quite
different in sparsity, ins shapes and their entropy numbers are so
close. To appreciate their difference, we should compare their
distributions by Jensen-Shannon measure (which is a symmetric). Or we
could use a representation of a hierarchical 2d-entropy, see Figure
\ref{fig:two}, where the entropy is split into 2x2, 4x4 and 8x8 (or
fewer if the distribution is not square). We have a hierarchical
entropy heat maps.

\doublefigure{.30}{../python/ENTOPF3754/2d-regular}{../python/ENTlposa07/2d-regular}{Hierarchical
  2D entropy for OPF 3754 (left) and LP OSA 07 (right). }{fig:two}

We can see a more granular entropy measure summarizes better the
nature of the matrix. In this work, the entropy vector is used mostly
for visualization purpose more than for comparison purpose. Of course,
we can appreciate how the matrix LP OSA 07 has a few very heavy rows
and they are clustered. This matrix will help up in showing how
randomization need some tips. Now we apply row and column random
permutation once by row and one by column: Figure \ref{fig:three}: OPF
has now entropy 11.27 and LP 9.26. The numerical difference is
significant. The good news is that for entropy, being an expectation,
we can use simple techniques like bootstrap to show that the
difference is significant or we have shown that Jensen-Shannon can be
used and a significance level is available. What we like to see is the
the hierarchical entropy heat map is becoming {\em more} uniform for
at least one of the matrix.

\doublefigure{.30}{../python/ENTOPF3754/2d-row-column-shuffle}{../python/ENTlposa07/2d-row-column-shuffle}{Hierarchical
  2D entropy after row and column random permutation for OPF 3754
  (left) and LP OSA 07 (right). }{fig:three}

In practice, permutation need some help especially for relatively
large matrices. As you can see, the permutation affects locally the
matrix. Of course, it depends on the implementation of the random
permutation (we use numpy for this) but it is reasonable a slightly
modified version of the original is still a random selection but
unfortunately they seem more likely than they should. We need to
compensate or help the randomization so that this current
implementation does not get too lazy.

If we are able to identify the row and column that divide high and low
density, we could use them as pivot for a shuffle like in a quick-sort
algorithm. We could apply a sorting algorithm but its complexity will
the same of SpMV. We use a gradients operations to choose the element
with maximum steepness, Figure \ref{fig:four} and \ref{fig:five}

LP achieves entropy 8.67 and 9.58 and OPF achieves 10.47 and 11.40.

\doublefigure{.30}{../python/ENTOPF3754/2d-H-shuffle}{../python/ENTlposa07/2d-H-shuffle}{Hierarchical
  2D entropy after height gradient based shuffle and row random
  permutation for OPF 3754 (left) and LP OSA 07 (right). }{fig:four}

\doublefigure{.30}{../python/ENTOPF3754/2d-W-shuffle}{../python/ENTlposa07/2d-W-shuffle}{Hierarchical
  2D entropy after height and width gradient shuffle and row and
  column random permutation for OPF 3754 (left) and LP OSA 07
  (right). }{fig:five}

If the goal is to achieve a uniform matrix sparsity, it seems that we
have the basic tool to compute and to measure such a sparsity. We
admit that we do not try to find the best permutation. But our real
goal is to create a work bench where randomization can be tested on
different architectures and different algorithms.

\section{Measuring the randomization effects}
\label{sec:measuring}

Whether or not this applied to the reader, when we have timed
execution of algorithms we came to expect variation.  The introduction
of randomization may hide behind the ever present random behavior,
after all these are algorithms on {\em small} inputs and small error
can be comparable to the overall execution time. Here, we must address
this concern even before describing the experiments.

First, every algorithm is run between 1000 and 5000 times. The time of
each experiments is in the seconds, providing a granularity we are
confident that error in measuring time (per se) is under
control. Thus, for each experiment we provide an average execution
time: we measure the time and we divide by the number of trials. Cold
starts, the first iteration, are still accounted. To make the measure
portable across platform we present GFLOPS, that is, Giga ($10^12$)
floating operations per second: $2*nnz$ divided by the average time in
seconds.

Then we repeat the same experiment 32 times. Permutations in {\em
  numpy} Python use a seed that time sensitive and thus every
experiment is independent from the previous. The number 32 is an old
statistic trick and it is a minimum number of independent trials to
approximate an normal distribution. In practice, they are not but the
number is sufficient for most of the cases and it is an excellent
starting point.

A short legend: {\bf Reg} is the matrix without any permutation and
thus is the regular; {\bf R} stands for random Row permutation; {\bf
  G-R} stands for gradient-based row shuffle and random row
permutation; {\bf G-C} stands for gradient-based column shuffle and
random column permutation; {\bf R-C} stands for random row and column
permutation. Gradient based approach shall we be clarified further in
the experimental results section
\ref{sec:experimentalresults}. Intuitively, we help the random
permutation by a quick targeting of high and low volume of the matrix.


In Figure \ref{fig:five}, We show CPU performance
using COO and CSR SpMV algorithms for the matrix OPF 3754. We can see
that the CSR algorithms are consistent and the Regular (i.e., the
original) has always the best performance. For the COO, permutations
introduce a long tails. In Figure \ref{fig:six}, Randomization is
harmful to the GPU implementation. If the load balance is fixed (i.e.,
by dividing the matrix by row and in equal row), randomization is
beneficial.

\doublefigure{.45}{../python/PNG2/OPF_3754_mtx_CPU_COO.png}{../python/PNG2/OPF_3754_mtx_CPU_CSR.png}{CPU COO (left) and CPU CSR (left) for OPF 3754}{fig:five}

\doublefigure{.45}{../python/PNG2/OPF_3754_mtx_GPU_64_COO.png}{../python/PNG2/OPF_3754_mtx_GPU_64_CSR.png}{GPU 64bits COO (left) and GPU CSR (left) for OPF 3754}{fig:six}

\singlefigure{.45}{../python/PNG2/OPF_3754_mtx_CPU_PAR.png}{ Parallel
  CPU CSR (left) for OPF 3754}{fig:seven}

For matrix LP OSA 07, randomization helps clearly only for CPU CSR as
we show in Figure \ref{fig:eight}

\singlefigure{.45}{../python/PNG2/lp_osa_07_mtx_CPU_CSR.png}{   CPU CSR (left) for LP OSA 07}{fig:eight}

An example, the matrix MULT DCOP 01, is where randomization is useful
for the CPU, GPU, and the parallel version Figure \ref{fig:9},
\ref{fig:10}, and \ref{fig:11}. 

\doublefigure{.45}{../python/PNG2/mult_dcop_01_mtx_CPU_COO.png}{../python/PNG2/mult_dcop_01_mtx_CPU_CSR.png}{CPU COO (left) and CPU CSR (left) for MULT DCOP 01}{fig:9}

\doublefigure{.45}{../python/PNG2/mult_dcop_01_mtx_GPU_64_COO.png}{../python/PNG2/mult_dcop_01_mtx_GPU_64_CSR.png}{GPU 64bits COO (left) and GPU CSR (left) for MULT DCOP 01}{fig:10}

\singlefigure{.45}{../python/PNG2/mult_dcop_01_mtx_CPU_PAR.png}{ Parallel
  CPU CSR (left) for MULT DCOP 01 }{fig:11}


\section{Workloads}
\label{sec:workload}

In the previous sections, we defined what we mean for randomization
and we present our tools of tricks for the measure of the effects of
randomization. Here we describe the work loads, the applications, we
use to test the effects of the randomization.

\subsection{Python COO and CSR algorithms}

The simplicity to compute the SpMV by the code $z = A*b$ in Python is
very rewarding. By change of the matrix storage format, $AC =
A.tocsr(); z = AC*b$, we have a different algorithm. The performance
exploitation is moved to the lower level.  The CSR implementation is
often two times faster but there are edges cases where the COO and COO
with randomization can go beyond and be surprisingly better: MUL DCOP
03 is an example where COO can do well.

Intuitively, Randomization can affect the performance because the
basic implementation is a sorting algorithm and it is a fixed
algorithm. There are many sorting algorithms and each can be optimal
for a different initial distribution. If we knew what is the sorting
algorithm we could tailor the input distribution. Here we just play
with it.

\subsection{Parallel CSR using up to 16 cores}
Python provides the concept of Pool to exploit a naive parallel
computation. We notice that work given to a Pool are split accordingly
to the number of elements to separate HW cores. We also noticed that
the work load can move from a core to another, thus may not be
ideal. Also we notice that Pool introduce a noticeable overhead: a
Pool of 1, never achieves the performance of the single thread $z =
AC*b$. Using Pool allows us to investigate how a naive row
partitioning without counting can scale up with number of
cores. Randomization goal is to distribute the work uniformly: a
balanced work distribution avoid the unfortunate case where a single
core does all the work.


\subsection{GPU COO and CSR algorithms}
In this work, we use AMD GPUs and {\em rocSPARSE} is their current
software. The software has a few glitches but overall can be used for
different generation of AMD GPUs. We use the COO and CSR algorithms
and when possible or useful we provide performance measure for single
and double precision (mostly double precision). The ideas of using
different GPUs is important to verify that the randomization can be
applied independently of the HW. We are not here to compare
performance with other GPUs or even between CPUs and GPUs.

The performance of the CSR algorithm is about two time faster than the
COO. Most of the algorithms use the CSR format to count the number of
sparse elements in a row and thus they can decide the work load
partition accordingly. Counting give you an edge but without changing
the order of the computation there could be cases where the work load
is not balanced and a little randomization could help and it helps.

%\subsection{ FPGA ? (not necessary)}
\subsection{Randomization sometimes works}

For the majority of the cases we investigated and reported in the
following sections, Randomization does not work and it affects the
performance negatively. However, there are cases that do work and do
work for different algorithms and architectures. If you are in the
business of preconditioning, permutations are pretty cheap. Of course,
permutation changes the computation order and this may affect
precision: for low precision matrices such as half floating point
(fp16) or smaller we may re-evaluate. For the semiring (min,+) and for
integer arithmetic the computation order does not matter.



\section{Call for a different strategy}
\label{sec:strategy}
We want to find out randomization techniques that are suitable for
custom hardware but also what are the most common and simple
heuristics that can justified for any hardware.


\section{Experimental Results}
\label{sec:experimentalresults}
Plots and pots.
\input{out.v.tex}




%\input{conclusion.tex}

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ACM-Reference-Format} \bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix{Review and Response}
%\input{review.tex}
\end{document}
